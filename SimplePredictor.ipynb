{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimplePredictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LGQnYI2nxvF"
      },
      "source": [
        "This simple predictor introduces concepts like weights, bias, hidden nodes, activation function, learning rate, loss and gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKQQ7-mcbgKW",
        "outputId": "036ce942-9528-4d33-941c-164b4f5f9b8c"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.1):\n",
        "\n",
        "      dw = 0.1\n",
        "      db = 0.1\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + dw\n",
        "        b = b + db\n",
        "      else:\n",
        "        w = w - dw\n",
        "        b = b - db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0999999999999996\n",
            "Original Output :  4, Predicted Output :  3.9999999999999982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAT2wE4KrTrx"
      },
      "source": [
        "We were close enough. But what will happen if we try to minimize the loss to a point less than 0.1. Let us see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hxxh9cKgl2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96639265-cdcd-4437-8b77-39a7f0f5eb24"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.01):\n",
        "\n",
        "      dw = 0.1\n",
        "      db = 0.1\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + dw\n",
        "        b = b + db\n",
        "      else:\n",
        "        w = w - dw\n",
        "        b = b - db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0099999999999993\n",
            "Original Output :  4, Predicted Output :  3.9999999999999982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KaocQlso9Uw"
      },
      "source": [
        "It is not working. Because we have not introduce any learning rate and so, we are optimizing weights and biases by 0.1 at each iteration. So, we were jumping over the minimum and our loss were always greater than 0.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFfmB8rp5XF"
      },
      "source": [
        "To solve this, let us minimize the weights and biases by 0.01 instead of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlwItXMcoFtX",
        "outputId": "ff0b63dd-e1db-4e36-eb87-d07a7fef94a8"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.01):\n",
        "\n",
        "      dw = 0.01\n",
        "      db = 0.01\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + dw\n",
        "        b = b + db\n",
        "      else:\n",
        "        w = w - dw\n",
        "        b = b - db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0099999999999993\n",
            "Original Output :  4, Predicted Output :  3.9999999999999982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdkZyPZEqIES"
      },
      "source": [
        "We can see that it is working now. Instead of hard coding 0.01, we can introduce a variable called learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axe3BqhuqGJc",
        "outputId": "70c5cbb4-f2b7-4667-cd9e-7859b41a7e6e"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.01):\n",
        "\n",
        "      dw = 0.1\n",
        "      db = 0.1\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + learning_rate*dw\n",
        "        b = b + learning_rate*db\n",
        "      else:\n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0099999999999993\n",
            "Original Output :  4, Predicted Output :  3.999999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVIiVnDYqG0o"
      },
      "source": [
        "As we can see, it is working."
      ]
    }
  ]
}