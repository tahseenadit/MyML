{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimplePredictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LGQnYI2nxvF"
      },
      "source": [
        "This simple predictor introduces concepts like weights, bias, hidden nodes, activation function, learning rate, loss and gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKQQ7-mcbgKW",
        "outputId": "036ce942-9528-4d33-941c-164b4f5f9b8c"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.1):\n",
        "\n",
        "      dw = 0.1\n",
        "      db = 0.1\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + dw\n",
        "        b = b + db\n",
        "      else:\n",
        "        w = w - dw\n",
        "        b = b - db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0999999999999996\n",
            "Original Output :  4, Predicted Output :  3.9999999999999982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAT2wE4KrTrx"
      },
      "source": [
        "We were close enough. But what will happen if we try to minimize the loss to a point less than 0.1. Let us see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hxxh9cKgl2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96639265-cdcd-4437-8b77-39a7f0f5eb24"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.01):\n",
        "\n",
        "      dw = 0.1\n",
        "      db = 0.1\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + dw\n",
        "        b = b + db\n",
        "      else:\n",
        "        w = w - dw\n",
        "        b = b - db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0099999999999993\n",
            "Original Output :  4, Predicted Output :  3.9999999999999982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KaocQlso9Uw"
      },
      "source": [
        "It is not working. Because we have not introduce any learning rate and so, we are optimizing weights and biases by 0.1 at each iteration. So, we were jumping over the minimum and our loss were always greater than 0.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFfmB8rp5XF"
      },
      "source": [
        "To solve this, let us minimize the weights and biases by 0.01 instead of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlwItXMcoFtX",
        "outputId": "ff0b63dd-e1db-4e36-eb87-d07a7fef94a8"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.01):\n",
        "\n",
        "      dw = 0.01\n",
        "      db = 0.01\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + dw\n",
        "        b = b + db\n",
        "      else:\n",
        "        w = w - dw\n",
        "        b = b - db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0099999999999993\n",
            "Original Output :  4, Predicted Output :  3.9999999999999982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdkZyPZEqIES"
      },
      "source": [
        "We can see that it is working now. Instead of hard coding 0.01, we can introduce a variable called learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axe3BqhuqGJc",
        "outputId": "70c5cbb4-f2b7-4667-cd9e-7859b41a7e6e"
      },
      "source": [
        "simple_dataset = [1,2,3]\n",
        "initial_weight = 2\n",
        "initial_bias = 0\n",
        "\n",
        "def activation_function(h):\n",
        "    return h\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "for x in simple_dataset:\n",
        "\n",
        "  original_output = x + 1\n",
        "\n",
        "  print(\"Original Output : \", original_output, end=\", \")\n",
        "\n",
        "  #  First compute the hidden node.\n",
        "  h = initial_weight*x + initial_bias\n",
        "\n",
        "  # Pass the output of the hidden node to the activation function\n",
        "  a = activation_function(h)\n",
        "\n",
        "  # The output of the activation function is our predicted output.\n",
        "  predicted_output = a\n",
        "\n",
        "  # Find the loss\n",
        "  loss = abs(original_output - predicted_output)\n",
        "\n",
        "  # Perform multiple steps for single data to minimize the loss of the prediction for that.\n",
        "  w = initial_weight\n",
        "  b = initial_bias\n",
        "\n",
        "  while(loss > 0.01):\n",
        "\n",
        "      dw = 0.1\n",
        "      db = 0.1\n",
        "\n",
        "      if (original_output - predicted_output > 0 ):\n",
        "        w = w + learning_rate*dw\n",
        "        b = b + learning_rate*db\n",
        "      else:\n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "\n",
        "      h = w*x + b\n",
        "      a = activation_function(h)\n",
        "      predicted_output = a\n",
        "\n",
        "      loss = abs(original_output - predicted_output)\n",
        "\n",
        "  print(\"Predicted Output : \", predicted_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Output :  2, Predicted Output :  2\n",
            "Original Output :  3, Predicted Output :  3.0099999999999993\n",
            "Original Output :  4, Predicted Output :  3.999999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVIiVnDYqG0o"
      },
      "source": [
        "As we can see, it is working."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJSttE-tt1B_"
      },
      "source": [
        "But in the avobe examples, we are not actually implementing the concept of epochs currectly. We are so far taking a single input from the input vector and predicting the output, then taking another input from the input vector and for that new input vector, we are taking fresh initial weight and bias and again calculating the output for that input vector. We are calculating loss for each input vector.\r\n",
        "\r\n",
        "But in one epoch, we should only take the initial weight and bias once, keep them updating, calculating loss for each input, take the average loss and based on that we should update our initial weight and bias and start the next epoch with that updated weight and bias.\r\n",
        "\r\n",
        "Let us do it like that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taiNvQRRu6gr",
        "outputId": "ec3a7692-8dcf-4991-f9c4-f2b471966194"
      },
      "source": [
        "import statistics\r\n",
        "\r\n",
        "simple_dataset = [1,2,3,4,5]\r\n",
        "initial_weight = 2\r\n",
        "initial_bias = 0\r\n",
        "\r\n",
        "def activation_function(h):\r\n",
        "    return h\r\n",
        "\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "# Perform multiple steps for single data to minimize the loss of the prediction for that.\r\n",
        "w = initial_weight\r\n",
        "b = initial_bias\r\n",
        "dw = 0.1\r\n",
        "db = 0.1\r\n",
        "losses = []\r\n",
        "\r\n",
        "for epoch in range(200):\r\n",
        "  print(\"Epoch : \", epoch, \", \", end=\"\")\r\n",
        "  for x in simple_dataset:\r\n",
        "\r\n",
        "    original_output = x + 1\r\n",
        "\r\n",
        "    # print(\"Original Output : \", original_output, end=\", \")\r\n",
        "\r\n",
        "    #  First compute the hidden node.\r\n",
        "    h = w*x + b\r\n",
        "\r\n",
        "    # Pass the output of the hidden node to the activation function\r\n",
        "    a = activation_function(h)\r\n",
        "\r\n",
        "    # The output of the activation function is our predicted output.\r\n",
        "    predicted_output = a\r\n",
        "\r\n",
        "    # Find the loss\r\n",
        "    loss = (original_output - predicted_output)\r\n",
        "\r\n",
        "    losses.append(loss)\r\n",
        "\r\n",
        "  #   print(\"Predicted Output: \", predicted_output)\r\n",
        "  # print(\"\\n\")\r\n",
        "\r\n",
        "  if (statistics.mean(losses) > 0 ):\r\n",
        "    w = w + learning_rate*dw\r\n",
        "    b = b + learning_rate*db\r\n",
        "  else:\r\n",
        "    w = w - learning_rate*dw\r\n",
        "    b = b - learning_rate*db\r\n",
        "\r\n",
        "  print(\"avg loss: \", statistics.mean(losses))\r\n",
        "\r\n",
        "# Let us predict now\r\n",
        "\r\n",
        "x = 6\r\n",
        "#  First compute the hidden node.\r\n",
        "h = w*x + b\r\n",
        "\r\n",
        "# Pass the output of the hidden node to the activation function\r\n",
        "a = activation_function(h)\r\n",
        "\r\n",
        "# The output of the activation function is our predicted output.\r\n",
        "predicted_output = a\r\n",
        "print(predicted_output)\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  0 , avg loss:  -2\n",
            "Epoch :  1 , avg loss:  -1.98\n",
            "Epoch :  2 , avg loss:  -1.96\n",
            "Epoch :  3 , avg loss:  -1.94\n",
            "Epoch :  4 , avg loss:  -1.9200000000000002\n",
            "Epoch :  5 , avg loss:  -1.9000000000000001\n",
            "Epoch :  6 , avg loss:  -1.8800000000000001\n",
            "Epoch :  7 , avg loss:  -1.8599999999999999\n",
            "Epoch :  8 , avg loss:  -1.8399999999999999\n",
            "Epoch :  9 , avg loss:  -1.8199999999999998\n",
            "Epoch :  10 , avg loss:  -1.7999999999999998\n",
            "Epoch :  11 , avg loss:  -1.7799999999999998\n",
            "Epoch :  12 , avg loss:  -1.7599999999999998\n",
            "Epoch :  13 , avg loss:  -1.7399999999999998\n",
            "Epoch :  14 , avg loss:  -1.7199999999999998\n",
            "Epoch :  15 , avg loss:  -1.6999999999999997\n",
            "Epoch :  16 , avg loss:  -1.6799999999999997\n",
            "Epoch :  17 , avg loss:  -1.6599999999999997\n",
            "Epoch :  18 , avg loss:  -1.6399999999999997\n",
            "Epoch :  19 , avg loss:  -1.6199999999999997\n",
            "Epoch :  20 , avg loss:  -1.5999999999999996\n",
            "Epoch :  21 , avg loss:  -1.5799999999999996\n",
            "Epoch :  22 , avg loss:  -1.5599999999999996\n",
            "Epoch :  23 , avg loss:  -1.5399999999999996\n",
            "Epoch :  24 , avg loss:  -1.5199999999999996\n",
            "Epoch :  25 , avg loss:  -1.4999999999999996\n",
            "Epoch :  26 , avg loss:  -1.4799999999999995\n",
            "Epoch :  27 , avg loss:  -1.4599999999999995\n",
            "Epoch :  28 , avg loss:  -1.4399999999999995\n",
            "Epoch :  29 , avg loss:  -1.4199999999999995\n",
            "Epoch :  30 , avg loss:  -1.3999999999999995\n",
            "Epoch :  31 , avg loss:  -1.3799999999999994\n",
            "Epoch :  32 , avg loss:  -1.3599999999999994\n",
            "Epoch :  33 , avg loss:  -1.3399999999999994\n",
            "Epoch :  34 , avg loss:  -1.3199999999999994\n",
            "Epoch :  35 , avg loss:  -1.2999999999999994\n",
            "Epoch :  36 , avg loss:  -1.2799999999999994\n",
            "Epoch :  37 , avg loss:  -1.2599999999999993\n",
            "Epoch :  38 , avg loss:  -1.2399999999999993\n",
            "Epoch :  39 , avg loss:  -1.2199999999999993\n",
            "Epoch :  40 , avg loss:  -1.1999999999999993\n",
            "Epoch :  41 , avg loss:  -1.1799999999999993\n",
            "Epoch :  42 , avg loss:  -1.1599999999999993\n",
            "Epoch :  43 , avg loss:  -1.1399999999999992\n",
            "Epoch :  44 , avg loss:  -1.1199999999999992\n",
            "Epoch :  45 , avg loss:  -1.0999999999999992\n",
            "Epoch :  46 , avg loss:  -1.0799999999999992\n",
            "Epoch :  47 , avg loss:  -1.0599999999999992\n",
            "Epoch :  48 , avg loss:  -1.0399999999999991\n",
            "Epoch :  49 , avg loss:  -1.0199999999999991\n",
            "Epoch :  50 , avg loss:  -0.9999999999999992\n",
            "Epoch :  51 , avg loss:  -0.9799999999999992\n",
            "Epoch :  52 , avg loss:  -0.9599999999999992\n",
            "Epoch :  53 , avg loss:  -0.9399999999999992\n",
            "Epoch :  54 , avg loss:  -0.9199999999999992\n",
            "Epoch :  55 , avg loss:  -0.8999999999999991\n",
            "Epoch :  56 , avg loss:  -0.8799999999999991\n",
            "Epoch :  57 , avg loss:  -0.8599999999999991\n",
            "Epoch :  58 , avg loss:  -0.8399999999999991\n",
            "Epoch :  59 , avg loss:  -0.8199999999999991\n",
            "Epoch :  60 , avg loss:  -0.799999999999999\n",
            "Epoch :  61 , avg loss:  -0.779999999999999\n",
            "Epoch :  62 , avg loss:  -0.759999999999999\n",
            "Epoch :  63 , avg loss:  -0.739999999999999\n",
            "Epoch :  64 , avg loss:  -0.719999999999999\n",
            "Epoch :  65 , avg loss:  -0.699999999999999\n",
            "Epoch :  66 , avg loss:  -0.6799999999999989\n",
            "Epoch :  67 , avg loss:  -0.6599999999999989\n",
            "Epoch :  68 , avg loss:  -0.6399999999999989\n",
            "Epoch :  69 , avg loss:  -0.6199999999999989\n",
            "Epoch :  70 , avg loss:  -0.5999999999999989\n",
            "Epoch :  71 , avg loss:  -0.5799999999999988\n",
            "Epoch :  72 , avg loss:  -0.5599999999999988\n",
            "Epoch :  73 , avg loss:  -0.5399999999999988\n",
            "Epoch :  74 , avg loss:  -0.5199999999999988\n",
            "Epoch :  75 , avg loss:  -0.4999999999999988\n",
            "Epoch :  76 , avg loss:  -0.47999999999999876\n",
            "Epoch :  77 , avg loss:  -0.45999999999999874\n",
            "Epoch :  78 , avg loss:  -0.4399999999999987\n",
            "Epoch :  79 , avg loss:  -0.4199999999999987\n",
            "Epoch :  80 , avg loss:  -0.3999999999999987\n",
            "Epoch :  81 , avg loss:  -0.37999999999999867\n",
            "Epoch :  82 , avg loss:  -0.35999999999999865\n",
            "Epoch :  83 , avg loss:  -0.33999999999999864\n",
            "Epoch :  84 , avg loss:  -0.3199999999999986\n",
            "Epoch :  85 , avg loss:  -0.2999999999999986\n",
            "Epoch :  86 , avg loss:  -0.2799999999999986\n",
            "Epoch :  87 , avg loss:  -0.25999999999999857\n",
            "Epoch :  88 , avg loss:  -0.23999999999999852\n",
            "Epoch :  89 , avg loss:  -0.2199999999999985\n",
            "Epoch :  90 , avg loss:  -0.19999999999999848\n",
            "Epoch :  91 , avg loss:  -0.17999999999999847\n",
            "Epoch :  92 , avg loss:  -0.15999999999999845\n",
            "Epoch :  93 , avg loss:  -0.13999999999999843\n",
            "Epoch :  94 , avg loss:  -0.11999999999999841\n",
            "Epoch :  95 , avg loss:  -0.0999999999999984\n",
            "Epoch :  96 , avg loss:  -0.07999999999999838\n",
            "Epoch :  97 , avg loss:  -0.05999999999999837\n",
            "Epoch :  98 , avg loss:  -0.03999999999999835\n",
            "Epoch :  99 , avg loss:  -0.01999999999999833\n",
            "Epoch :  100 , avg loss:  1.6866596128562773e-15\n",
            "Epoch :  101 , avg loss:  0.019215686274511506\n",
            "Epoch :  102 , avg loss:  0.03766990291262307\n",
            "Epoch :  103 , avg loss:  0.055384615384617114\n",
            "Epoch :  104 , avg loss:  0.07238095238095413\n",
            "Epoch :  105 , avg loss:  0.08867924528302062\n",
            "Epoch :  106 , avg loss:  0.10429906542056251\n",
            "Epoch :  107 , avg loss:  0.11925925925926104\n",
            "Epoch :  108 , avg loss:  0.13357798165137794\n",
            "Epoch :  109 , avg loss:  0.14727272727272908\n",
            "Epoch :  110 , avg loss:  0.16036036036036216\n",
            "Epoch :  111 , avg loss:  0.17285714285714468\n",
            "Epoch :  112 , avg loss:  0.18477876106194874\n",
            "Epoch :  113 , avg loss:  0.19614035087719484\n",
            "Epoch :  114 , avg loss:  0.2069565217391323\n",
            "Epoch :  115 , avg loss:  0.21724137931034668\n",
            "Epoch :  116 , avg loss:  0.2270085470085489\n",
            "Epoch :  117 , avg loss:  0.23627118644067985\n",
            "Epoch :  118 , avg loss:  0.24504201680672458\n",
            "Epoch :  119 , avg loss:  0.25333333333333524\n",
            "Epoch :  120 , avg loss:  0.2611570247933903\n",
            "Epoch :  121 , avg loss:  0.2685245901639363\n",
            "Epoch :  122 , avg loss:  0.2754471544715466\n",
            "Epoch :  123 , avg loss:  0.28193548387096967\n",
            "Epoch :  124 , avg loss:  0.2880000000000019\n",
            "Epoch :  125 , avg loss:  0.29365079365079555\n",
            "Epoch :  126 , avg loss:  0.2988976377952775\n",
            "Epoch :  127 , avg loss:  0.3037500000000019\n",
            "Epoch :  128 , avg loss:  0.3082170542635678\n",
            "Epoch :  129 , avg loss:  0.31230769230769423\n",
            "Epoch :  130 , avg loss:  0.316030534351147\n",
            "Epoch :  131 , avg loss:  0.31939393939394134\n",
            "Epoch :  132 , avg loss:  0.3224060150375959\n",
            "Epoch :  133 , avg loss:  0.32507462686567357\n",
            "Epoch :  134 , avg loss:  0.32740740740740937\n",
            "Epoch :  135 , avg loss:  0.3294117647058843\n",
            "Epoch :  136 , avg loss:  0.33109489051095087\n",
            "Epoch :  137 , avg loss:  0.33246376811594397\n",
            "Epoch :  138 , avg loss:  0.33352517985611707\n",
            "Epoch :  139 , avg loss:  0.33428571428571624\n",
            "Epoch :  140 , avg loss:  0.3347517730496473\n",
            "Epoch :  141 , avg loss:  0.3349295774647907\n",
            "Epoch :  142 , avg loss:  0.33482517482517676\n",
            "Epoch :  143 , avg loss:  0.3344444444444464\n",
            "Epoch :  144 , avg loss:  0.3337931034482778\n",
            "Epoch :  145 , avg loss:  0.3328767123287691\n",
            "Epoch :  146 , avg loss:  0.3317006802721108\n",
            "Epoch :  147 , avg loss:  0.3302702702702722\n",
            "Epoch :  148 , avg loss:  0.3285906040268476\n",
            "Epoch :  149 , avg loss:  0.3266666666666686\n",
            "Epoch :  150 , avg loss:  0.3245033112582801\n",
            "Epoch :  151 , avg loss:  0.32210526315789667\n",
            "Epoch :  152 , avg loss:  0.31947712418300844\n",
            "Epoch :  153 , avg loss:  0.3166233766233785\n",
            "Epoch :  154 , avg loss:  0.3135483870967761\n",
            "Epoch :  155 , avg loss:  0.31025641025641215\n",
            "Epoch :  156 , avg loss:  0.3067515923566898\n",
            "Epoch :  157 , avg loss:  0.3030379746835462\n",
            "Epoch :  158 , avg loss:  0.29911949685534783\n",
            "Epoch :  159 , avg loss:  0.29500000000000187\n",
            "Epoch :  160 , avg loss:  0.2906832298136665\n",
            "Epoch :  161 , avg loss:  0.2861728395061747\n",
            "Epoch :  162 , avg loss:  0.2814723926380387\n",
            "Epoch :  163 , avg loss:  0.2765853658536604\n",
            "Epoch :  164 , avg loss:  0.2715151515151534\n",
            "Epoch :  165 , avg loss:  0.26626506024096575\n",
            "Epoch :  166 , avg loss:  0.26083832335329526\n",
            "Epoch :  167 , avg loss:  0.2552380952380971\n",
            "Epoch :  168 , avg loss:  0.24946745562130362\n",
            "Epoch :  169 , avg loss:  0.24352941176470772\n",
            "Epoch :  170 , avg loss:  0.23742690058479715\n",
            "Epoch :  171 , avg loss:  0.23116279069767626\n",
            "Epoch :  172 , avg loss:  0.22473988439306541\n",
            "Epoch :  173 , avg loss:  0.2181609195402317\n",
            "Epoch :  174 , avg loss:  0.21142857142857324\n",
            "Epoch :  175 , avg loss:  0.20454545454545636\n",
            "Epoch :  176 , avg loss:  0.19751412429378712\n",
            "Epoch :  177 , avg loss:  0.1903370786516872\n",
            "Epoch :  178 , avg loss:  0.18301675977653808\n",
            "Epoch :  179 , avg loss:  0.17555555555555732\n",
            "Epoch :  180 , avg loss:  0.16795580110497416\n",
            "Epoch :  181 , avg loss:  0.16021978021978198\n",
            "Epoch :  182 , avg loss:  0.15234972677595804\n",
            "Epoch :  183 , avg loss:  0.14434782608695826\n",
            "Epoch :  184 , avg loss:  0.13621621621621796\n",
            "Epoch :  185 , avg loss:  0.12795698924731355\n",
            "Epoch :  186 , avg loss:  0.11957219251337071\n",
            "Epoch :  187 , avg loss:  0.11106382978723577\n",
            "Epoch :  188 , avg loss:  0.10243386243386414\n",
            "Epoch :  189 , avg loss:  0.0936842105263175\n",
            "Epoch :  190 , avg loss:  0.08481675392670326\n",
            "Epoch :  191 , avg loss:  0.07583333333333502\n",
            "Epoch :  192 , avg loss:  0.06673575129533847\n",
            "Epoch :  193 , avg loss:  0.057525773195877955\n",
            "Epoch :  194 , avg loss:  0.048205128205129864\n",
            "Epoch :  195 , avg loss:  0.038775510204083284\n",
            "Epoch :  196 , avg loss:  0.02923857868020469\n",
            "Epoch :  197 , avg loss:  0.01959595959596123\n",
            "Epoch :  198 , avg loss:  0.009849246231157403\n",
            "Epoch :  199 , avg loss:  1.6162626792493029e-15\n",
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yntsHyA1cNo"
      },
      "source": [
        "Let us introduce early stopping now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpmOyxfTwrWY",
        "outputId": "4894b206-cff1-4089-a4ed-c6e4f35b5076"
      },
      "source": [
        "import statistics\r\n",
        "\r\n",
        "simple_dataset = [1,2,3,4,5]\r\n",
        "initial_weight = 0.1\r\n",
        "initial_bias = 0.1\r\n",
        "\r\n",
        "def activation_function(h):\r\n",
        "    return h\r\n",
        "\r\n",
        "learning_rate = 0.1\r\n",
        "\r\n",
        "# Perform multiple steps for single data to minimize the loss of the prediction for that.\r\n",
        "w = initial_weight\r\n",
        "b = initial_bias\r\n",
        "dw = 0.1\r\n",
        "db = 0.1\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(200):\r\n",
        "  losses = []\r\n",
        "  print(\"Epoch : \", epoch, \", \", end=\"\")\r\n",
        "  for x in simple_dataset:\r\n",
        "\r\n",
        "    original_output = x + 1\r\n",
        "\r\n",
        "    #  First compute the hidden node.\r\n",
        "    h = w*x + b\r\n",
        "\r\n",
        "    # Pass the output of the hidden node to the activation function\r\n",
        "    a = activation_function(h)\r\n",
        "\r\n",
        "    # The output of the activation function is our predicted output.\r\n",
        "    predicted_output = a\r\n",
        "\r\n",
        "    # Find the loss\r\n",
        "    loss = (original_output - predicted_output)\r\n",
        "\r\n",
        "    losses.append(loss)\r\n",
        "\r\n",
        "  # early stopping\r\n",
        "  if (statistics.mean(losses) < 0.02):\r\n",
        "    if (statistics.mean(losses) > 0.01):\r\n",
        "      print(\"avg loss: \", statistics.mean(losses))\r\n",
        "      break\r\n",
        "\r\n",
        "  if (statistics.mean(losses) > 0 ):\r\n",
        "    w = w + learning_rate*dw\r\n",
        "    b = b + learning_rate*db\r\n",
        "  else:\r\n",
        "    w = w - learning_rate*dw\r\n",
        "    b = b - learning_rate*db\r\n",
        "\r\n",
        "  print(\"avg loss: \", statistics.mean(losses))\r\n",
        "  # print(\"w, b : \", w,b)\r\n",
        "\r\n",
        "# Let us predict now\r\n",
        "\r\n",
        "x = 600\r\n",
        "#  First compute the hidden node.\r\n",
        "h = w*x + b\r\n",
        "\r\n",
        "# Pass the output of the hidden node to the activation function\r\n",
        "a = activation_function(h)\r\n",
        "\r\n",
        "# The output of the activation function is our predicted output.\r\n",
        "predicted_output = a\r\n",
        "print(predicted_output)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  0 , avg loss:  3.6\n",
            "Epoch :  1 , avg loss:  3.56\n",
            "Epoch :  2 , avg loss:  3.52\n",
            "Epoch :  3 , avg loss:  3.48\n",
            "Epoch :  4 , avg loss:  3.44\n",
            "Epoch :  5 , avg loss:  3.4\n",
            "Epoch :  6 , avg loss:  3.36\n",
            "Epoch :  7 , avg loss:  3.32\n",
            "Epoch :  8 , avg loss:  3.28\n",
            "Epoch :  9 , avg loss:  3.2399999999999998\n",
            "Epoch :  10 , avg loss:  3.1999999999999993\n",
            "Epoch :  11 , avg loss:  3.1599999999999997\n",
            "Epoch :  12 , avg loss:  3.1199999999999997\n",
            "Epoch :  13 , avg loss:  3.0799999999999996\n",
            "Epoch :  14 , avg loss:  3.039999999999999\n",
            "Epoch :  15 , avg loss:  2.9999999999999996\n",
            "Epoch :  16 , avg loss:  2.9599999999999995\n",
            "Epoch :  17 , avg loss:  2.9199999999999995\n",
            "Epoch :  18 , avg loss:  2.8799999999999994\n",
            "Epoch :  19 , avg loss:  2.8399999999999994\n",
            "Epoch :  20 , avg loss:  2.7999999999999994\n",
            "Epoch :  21 , avg loss:  2.7599999999999993\n",
            "Epoch :  22 , avg loss:  2.7199999999999993\n",
            "Epoch :  23 , avg loss:  2.6799999999999993\n",
            "Epoch :  24 , avg loss:  2.6399999999999992\n",
            "Epoch :  25 , avg loss:  2.599999999999999\n",
            "Epoch :  26 , avg loss:  2.559999999999999\n",
            "Epoch :  27 , avg loss:  2.519999999999999\n",
            "Epoch :  28 , avg loss:  2.479999999999999\n",
            "Epoch :  29 , avg loss:  2.439999999999999\n",
            "Epoch :  30 , avg loss:  2.399999999999999\n",
            "Epoch :  31 , avg loss:  2.359999999999999\n",
            "Epoch :  32 , avg loss:  2.319999999999999\n",
            "Epoch :  33 , avg loss:  2.279999999999999\n",
            "Epoch :  34 , avg loss:  2.239999999999999\n",
            "Epoch :  35 , avg loss:  2.199999999999999\n",
            "Epoch :  36 , avg loss:  2.159999999999999\n",
            "Epoch :  37 , avg loss:  2.1199999999999988\n",
            "Epoch :  38 , avg loss:  2.0799999999999987\n",
            "Epoch :  39 , avg loss:  2.0399999999999987\n",
            "Epoch :  40 , avg loss:  1.9999999999999987\n",
            "Epoch :  41 , avg loss:  1.9599999999999986\n",
            "Epoch :  42 , avg loss:  1.9199999999999986\n",
            "Epoch :  43 , avg loss:  1.8799999999999986\n",
            "Epoch :  44 , avg loss:  1.8399999999999985\n",
            "Epoch :  45 , avg loss:  1.7999999999999985\n",
            "Epoch :  46 , avg loss:  1.7599999999999985\n",
            "Epoch :  47 , avg loss:  1.7199999999999984\n",
            "Epoch :  48 , avg loss:  1.6799999999999984\n",
            "Epoch :  49 , avg loss:  1.6399999999999983\n",
            "Epoch :  50 , avg loss:  1.5999999999999983\n",
            "Epoch :  51 , avg loss:  1.5599999999999983\n",
            "Epoch :  52 , avg loss:  1.5199999999999982\n",
            "Epoch :  53 , avg loss:  1.4799999999999982\n",
            "Epoch :  54 , avg loss:  1.4399999999999982\n",
            "Epoch :  55 , avg loss:  1.3999999999999981\n",
            "Epoch :  56 , avg loss:  1.359999999999998\n",
            "Epoch :  57 , avg loss:  1.319999999999998\n",
            "Epoch :  58 , avg loss:  1.279999999999998\n",
            "Epoch :  59 , avg loss:  1.239999999999998\n",
            "Epoch :  60 , avg loss:  1.199999999999998\n",
            "Epoch :  61 , avg loss:  1.159999999999998\n",
            "Epoch :  62 , avg loss:  1.1199999999999979\n",
            "Epoch :  63 , avg loss:  1.0799999999999979\n",
            "Epoch :  64 , avg loss:  1.0399999999999978\n",
            "Epoch :  65 , avg loss:  0.9999999999999978\n",
            "Epoch :  66 , avg loss:  0.9599999999999977\n",
            "Epoch :  67 , avg loss:  0.9199999999999977\n",
            "Epoch :  68 , avg loss:  0.8799999999999977\n",
            "Epoch :  69 , avg loss:  0.8399999999999976\n",
            "Epoch :  70 , avg loss:  0.7999999999999977\n",
            "Epoch :  71 , avg loss:  0.7599999999999975\n",
            "Epoch :  72 , avg loss:  0.7199999999999975\n",
            "Epoch :  73 , avg loss:  0.6799999999999975\n",
            "Epoch :  74 , avg loss:  0.6399999999999976\n",
            "Epoch :  75 , avg loss:  0.5999999999999973\n",
            "Epoch :  76 , avg loss:  0.5599999999999974\n",
            "Epoch :  77 , avg loss:  0.5199999999999974\n",
            "Epoch :  78 , avg loss:  0.47999999999999743\n",
            "Epoch :  79 , avg loss:  0.43999999999999717\n",
            "Epoch :  80 , avg loss:  0.3999999999999973\n",
            "Epoch :  81 , avg loss:  0.35999999999999716\n",
            "Epoch :  82 , avg loss:  0.3199999999999973\n",
            "Epoch :  83 , avg loss:  0.27999999999999703\n",
            "Epoch :  84 , avg loss:  0.23999999999999716\n",
            "Epoch :  85 , avg loss:  0.199999999999997\n",
            "Epoch :  86 , avg loss:  0.15999999999999717\n",
            "Epoch :  87 , avg loss:  0.11999999999999686\n",
            "Epoch :  88 , avg loss:  0.079999999999997\n",
            "Epoch :  89 , avg loss:  0.039999999999996885\n",
            "Epoch :  90 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  91 , avg loss:  0.039999999999997635\n",
            "Epoch :  92 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  93 , avg loss:  0.039999999999997635\n",
            "Epoch :  94 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  95 , avg loss:  0.039999999999997635\n",
            "Epoch :  96 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  97 , avg loss:  0.039999999999997635\n",
            "Epoch :  98 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  99 , avg loss:  0.039999999999997635\n",
            "Epoch :  100 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  101 , avg loss:  0.039999999999997635\n",
            "Epoch :  102 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  103 , avg loss:  0.039999999999997635\n",
            "Epoch :  104 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  105 , avg loss:  0.039999999999997635\n",
            "Epoch :  106 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  107 , avg loss:  0.039999999999997635\n",
            "Epoch :  108 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  109 , avg loss:  0.039999999999997635\n",
            "Epoch :  110 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  111 , avg loss:  0.039999999999997635\n",
            "Epoch :  112 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  113 , avg loss:  0.039999999999997635\n",
            "Epoch :  114 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  115 , avg loss:  0.039999999999997635\n",
            "Epoch :  116 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  117 , avg loss:  0.039999999999997635\n",
            "Epoch :  118 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  119 , avg loss:  0.039999999999997635\n",
            "Epoch :  120 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  121 , avg loss:  0.039999999999997635\n",
            "Epoch :  122 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  123 , avg loss:  0.039999999999997635\n",
            "Epoch :  124 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  125 , avg loss:  0.039999999999997635\n",
            "Epoch :  126 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  127 , avg loss:  0.039999999999997635\n",
            "Epoch :  128 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  129 , avg loss:  0.039999999999997635\n",
            "Epoch :  130 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  131 , avg loss:  0.039999999999997635\n",
            "Epoch :  132 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  133 , avg loss:  0.039999999999997635\n",
            "Epoch :  134 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  135 , avg loss:  0.039999999999997635\n",
            "Epoch :  136 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  137 , avg loss:  0.039999999999997635\n",
            "Epoch :  138 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  139 , avg loss:  0.039999999999997635\n",
            "Epoch :  140 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  141 , avg loss:  0.039999999999997635\n",
            "Epoch :  142 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  143 , avg loss:  0.039999999999997635\n",
            "Epoch :  144 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  145 , avg loss:  0.039999999999997635\n",
            "Epoch :  146 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  147 , avg loss:  0.039999999999997635\n",
            "Epoch :  148 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  149 , avg loss:  0.039999999999997635\n",
            "Epoch :  150 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  151 , avg loss:  0.039999999999997635\n",
            "Epoch :  152 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  153 , avg loss:  0.039999999999997635\n",
            "Epoch :  154 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  155 , avg loss:  0.039999999999997635\n",
            "Epoch :  156 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  157 , avg loss:  0.039999999999997635\n",
            "Epoch :  158 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  159 , avg loss:  0.039999999999997635\n",
            "Epoch :  160 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  161 , avg loss:  0.039999999999997635\n",
            "Epoch :  162 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  163 , avg loss:  0.039999999999997635\n",
            "Epoch :  164 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  165 , avg loss:  0.039999999999997635\n",
            "Epoch :  166 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  167 , avg loss:  0.039999999999997635\n",
            "Epoch :  168 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  169 , avg loss:  0.039999999999997635\n",
            "Epoch :  170 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  171 , avg loss:  0.039999999999997635\n",
            "Epoch :  172 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  173 , avg loss:  0.039999999999997635\n",
            "Epoch :  174 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  175 , avg loss:  0.039999999999997635\n",
            "Epoch :  176 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  177 , avg loss:  0.039999999999997635\n",
            "Epoch :  178 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  179 , avg loss:  0.039999999999997635\n",
            "Epoch :  180 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  181 , avg loss:  0.039999999999997635\n",
            "Epoch :  182 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  183 , avg loss:  0.039999999999997635\n",
            "Epoch :  184 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  185 , avg loss:  0.039999999999997635\n",
            "Epoch :  186 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  187 , avg loss:  0.039999999999997635\n",
            "Epoch :  188 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  189 , avg loss:  0.039999999999997635\n",
            "Epoch :  190 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  191 , avg loss:  0.039999999999997635\n",
            "Epoch :  192 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  193 , avg loss:  0.039999999999997635\n",
            "Epoch :  194 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  195 , avg loss:  0.039999999999997635\n",
            "Epoch :  196 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  197 , avg loss:  0.039999999999997635\n",
            "Epoch :  198 , avg loss:  -2.7533531010703883e-15\n",
            "Epoch :  199 , avg loss:  0.039999999999997635\n",
            "601.0000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zna5Oua_11GY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}