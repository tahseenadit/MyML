{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "import numpy as np\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = tk.Tk()\n",
    "\n",
    "# canvas = turtle.ScrolledCanvas(root)\n",
    "# canvas.pack(side=tk.LEFT)\n",
    "\n",
    "# screen = turtle.TurtleScreen(canvas)\n",
    "# screen.setworldcoordinates(-10, 100, 100, -10)\n",
    "\n",
    "# turtle = turtle.RawTurtle(screen)\n",
    "# turtle.goto(90, 90)\n",
    "\n",
    "# screen.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the game window\n",
    "game_window = turtle.Screen()\n",
    "game_window.title(\"RL@Tahseen\")\n",
    "game_window.setup(width=800,height=600)\n",
    "game_window.bgcolor(\"black\")\n",
    "game_window.setworldcoordinates(0,0,800,600)\n",
    "# Stop window from updating automatically\n",
    "game_window.tracer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the road\n",
    "out_left_line = turtle.Turtle()\n",
    "out_left_line.speed(0) # set the animation speed to 0\n",
    "out_left_line.color(\"white\")\n",
    "out_left_line.shape(\"square\")\n",
    "out_left_line.shapesize(stretch_wid = 20,stretch_len = 1)\n",
    "out_left_line.penup()\n",
    "out_left_line.goto(100,300)\n",
    "\n",
    "in_left_line = turtle.Turtle()\n",
    "in_left_line.speed(0) # set the animation speed to 0\n",
    "in_left_line.color(\"white\")\n",
    "in_left_line.shape(\"square\")\n",
    "in_left_line.shapesize(stretch_wid = 10,stretch_len = 1)\n",
    "in_left_line.penup()\n",
    "in_left_line.goto(200,300)\n",
    "\n",
    "out_right_line = turtle.Turtle()\n",
    "out_right_line.speed(0) # set the animation speed to 0\n",
    "out_right_line.color(\"white\")\n",
    "out_right_line.shape(\"square\")\n",
    "out_right_line.shapesize(stretch_wid = 20,stretch_len=1)\n",
    "out_right_line.penup()\n",
    "out_right_line.goto(700,300)\n",
    "\n",
    "in_right_line = turtle.Turtle()\n",
    "in_right_line.speed(0) # set the animation speed to 0\n",
    "in_right_line.color(\"white\")\n",
    "in_right_line.shape(\"square\")\n",
    "in_right_line.shapesize(stretch_wid = 10,stretch_len = 1)\n",
    "in_right_line.penup()\n",
    "in_right_line.goto(600,300)\n",
    "\n",
    "up_in = turtle.Turtle()\n",
    "up_in.shape(\"square\")\n",
    "up_in.shapesize(stretch_wid=1,stretch_len=20)\n",
    "up_in.color(\"white\")\n",
    "up_in.penup()\n",
    "up_in.speed(0)\n",
    "up_in.goto(400,400)\n",
    "\n",
    "down_in = turtle.Turtle()\n",
    "down_in.shape(\"square\")\n",
    "down_in.shapesize(stretch_wid=1,stretch_len=20)\n",
    "down_in.color(\"white\")\n",
    "down_in.penup()\n",
    "down_in.speed(0)\n",
    "down_in.goto(400,200)\n",
    "\n",
    "up_out = turtle.Turtle()\n",
    "up_out.shape(\"square\")\n",
    "up_out.shapesize(stretch_wid=1,stretch_len=30)\n",
    "up_out.color(\"white\")\n",
    "up_out.penup()\n",
    "up_out.speed(0)\n",
    "up_out.goto(400,500)\n",
    "\n",
    "down_out = turtle.Turtle()\n",
    "down_out.shape(\"square\")\n",
    "down_out.shapesize(stretch_wid=1,stretch_len=30)\n",
    "down_out.color(\"white\")\n",
    "down_out.penup()\n",
    "down_out.speed(0)\n",
    "down_out.goto(400,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent\n",
    "\n",
    "agent = turtle.Turtle()\n",
    "agent.speed(0)\n",
    "agent.shape(\"square\")\n",
    "agent.penup()\n",
    "agent.color(\"red\")\n",
    "agent.shapesize(stretch_wid = 1, stretch_len=2)\n",
    "agent.goto(400,150)\n",
    "# move speed\n",
    "agent.dx = 1\n",
    "agent.dy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_window.reset()\n",
    "\n",
    "# # Get the starting co-ordinate of the agent\n",
    "start_x = agent.xcor()\n",
    "start_y = agent.ycor()\n",
    "\n",
    "# actions\n",
    "actions = [\"up\",\"down\",\"forward\",\"diagonal_forw_up\",\"diagonal_forw_down\"]\n",
    "possible_actions = len(actions)\n",
    "curr_state = [start_x, start_y]\n",
    "\n",
    "# set initial q-values of all states. Each state has 5 q-values for 5 actions that can be taken from that state.\n",
    "q_values = np.zeros((game_window.window_width(), game_window.window_height(),possible_actions))\n",
    "\n",
    "# set rewards of all states intially to -100\n",
    "rewards = np.full((game_window.window_width(), game_window.window_height()), -100.)\n",
    "\n",
    "\n",
    "# Set rewards of all states in the roads initially to -1\n",
    "rewards[120:200,120:500] = -1.\n",
    "rewards[620:700,120:500] = -1.\n",
    "rewards[200:620,120:200] = -1.\n",
    "rewards[200:620,420:500] = -1.\n",
    "rewards[240:300,130:200] = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next_action(agent_curr_state_x,agent_curr_state_y,epsilon):\n",
    "    # agent_curr_state_x should be the current row index\n",
    "    # agent_curr_state_y should be the current column index\n",
    "    \n",
    "    # Now select a random number and check if it is greater than epsilon or not\n",
    "    if np.random.random() < epsilon:\n",
    "        # return the maximum q-values out of the 5 q-values in the current state.\n",
    "        # So that from the current state, the agent can select that action with the highest q-value \n",
    "        return np.argmax(q_values[agent_curr_state_x,agent_curr_state_y])\n",
    "    else:\n",
    "        return np.random.randint(possible_actions) # select an action randomly from possible actions\n",
    "\n",
    "def get_next_state_location(agent_curr_state_x,agent_curr_state_y,next_action_to_take):\n",
    "    \n",
    "    next_state_x, next_state_y = agent_curr_state_x,agent_curr_state_y\n",
    "    \n",
    "    if actions[next_action_to_take] == \"up\":\n",
    "        next_state_x, next_state_y = agent_curr_state_x, agent_curr_state_y + agent.dy\n",
    "    elif actions[next_action_to_take] == \"down\":\n",
    "        next_state_x, next_state_y = agent_curr_state_x, agent_curr_state_y - agent.dy\n",
    "    elif actions[next_action_to_take] == \"forward\":\n",
    "        next_state_x, next_state_y = agent_curr_state_x + agent.dx, agent_curr_state_y\n",
    "    elif actions[next_action_to_take] == \"diagonal_forw_up\":\n",
    "        next_state_x, next_state_y = agent_curr_state_x + agent.dx, agent_curr_state_y + agent.dy\n",
    "    elif actions[next_action_to_take] == \"diagonal_forw_down\":\n",
    "        next_state_x, next_state_y = agent_curr_state_x + agent.dx, agent_curr_state_y - agent.dy\n",
    "    else:\n",
    "        print(\"Do not move!\")\n",
    "    \n",
    "    return next_state_x, next_state_y\n",
    "\n",
    "def is_terminal_state(agent_curr_loc_x, agent_curr_loc_y):\n",
    "    if rewards[agent_curr_loc_x,agent_curr_loc_y] == -1.:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Start Training\n",
    "\n",
    "    # How many times we will train the agent\n",
    "    episodes = 1000\n",
    "\n",
    "    # epsilon indicates the percentage of time when we should choose the best action instead of a random action\n",
    "    epsilon = 0.7 # 90% time select the best action\n",
    "\n",
    "    # discount factor for future state's q_values\n",
    "    discount_factor = 0.4\n",
    "\n",
    "    learning_rate = 0.8\n",
    "\n",
    "    for episode in range(episodes):\n",
    "#         time_steps = 0\n",
    "        print(\"Episode:\",end=\" \")\n",
    "        print(episode)\n",
    "        while not is_terminal_state(agent.xcor(),agent.ycor()):\n",
    "\n",
    "            # Select the action to take from the current state\n",
    "            next_action_to_take = choose_next_action(agent.xcor(),agent.ycor(),epsilon)\n",
    "\n",
    "#             print(\"Next Action:\", end=\" \")\n",
    "#             print(next_action_to_take)\n",
    "\n",
    "            # save the current state x and y coordinates\n",
    "            curr_state_x, curr_state_y = agent.xcor(), agent.ycor()\n",
    "#             print(\"Agent's current location:\", end=\" \")\n",
    "#             print(curr_state_x)\n",
    "#             print(curr_state_y)\n",
    "            # get the next state x and y coordinates\n",
    "            # the next state is the state where the agent will go after taking the action indicated by 'next_action_to_take'\n",
    "            next_state_x, next_state_y = get_next_state_location(curr_state_x,curr_state_y,next_action_to_take)\n",
    "            \n",
    "#             if next_state_y >= 600:\n",
    "#                 next_state_y = 599\n",
    "#             if next_state_x >= 800:\n",
    "#                 next_state_x = 799\n",
    "            \n",
    "            # Get the reward to go to that next state\n",
    "            reward = rewards[next_state_x,next_state_y]\n",
    "\n",
    "            # Now calculate the current q-value of the current state for that 'next_action_to_take' from the current state\n",
    "            current_q_value = q_values[curr_state_x,curr_state_y,next_action_to_take]\n",
    "\n",
    "            # Now in the next state, there are 5 actions. So there are 5 q-values in the next state and we know those q-values.\n",
    "            # We can not say now which action will be choosen from that next state in future, but based on the current q-values\n",
    "            # of the next state, we can guess what can be the maximum q-value that we can achieve in that next state in future \n",
    "            # time step. Let's get that maximum q-value in the next state.\n",
    "\n",
    "            max_achievable_q_values_in_next_state = np.max(q_values[next_state_x,next_state_y])\n",
    "\n",
    "            # We do not want to get the full effect of that knowledge to our current state. Let's minimize the effect.\n",
    "\n",
    "            max_achievable_q_values_in_next_state = discount_factor * max_achievable_q_values_in_next_state\n",
    "\n",
    "            # Now, let us calculate the overall profit the agent can get by going to that next state\n",
    "\n",
    "            profit = reward + max_achievable_q_values_in_next_state\n",
    "\n",
    "            # Let us calculate how the profit is compared to the profit in our curent state\n",
    "            temporal_difference = profit - current_q_value\n",
    "\n",
    "            # Let us update the current state profit or in other words, q-value. We do not want to be greedy.  \n",
    "            # So, we will not learn 100%.\n",
    "            new_q_value_of_the_current_state_for_the_next_action = current_q_value + (learning_rate * temporal_difference)\n",
    "            q_values[curr_state_x,curr_state_y,next_action_to_take] = new_q_value_of_the_current_state_for_the_next_action\n",
    "\n",
    "            # Update agents location now\n",
    "            agent.setx(next_state_x)\n",
    "            agent.sety(next_state_y)\n",
    "\n",
    "        agent.setx(start_x)\n",
    "        agent.sety(start_y)\n",
    "        \n",
    "    print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main game loop that will keep the game running, inside the loop we are updatin the scene manually\n",
    "while True:\n",
    "    game_window.update()\n",
    "    \n",
    "#     agent.setx(agent.xcor()+agent.dx)\n",
    "#     agent.sety(agent.ycor()+agent.dy)\n",
    "\n",
    "    current_x, current_y = 400, 150\n",
    "    \n",
    "    agent.setx(current_x)\n",
    "    agent.sety(current_y)\n",
    "    \n",
    "    print(current_x, end=\" \")\n",
    "    print(current_y)\n",
    "    \n",
    "    while not is_terminal_state(current_x, current_y):\n",
    "        next_action_to_take = choose_next_action(current_x,current_y,1.)\n",
    "        next_state_x, next_state_y = get_next_state_location(current_x,current_y,next_action_to_take)\n",
    "        agent.setx(next_state_x)\n",
    "        agent.sety(next_state_y)\n",
    "        print(next_state_x, end=\" \")\n",
    "        print(next_state_y)\n",
    "        current_x = next_state_x\n",
    "        current_y = next_state_y\n",
    "\n",
    "#     # Start Training\n",
    "\n",
    "#     # How many times we will train the agent\n",
    "#     episodes = 1000\n",
    "\n",
    "#     # epsilon indicates the percentage of time when we should choose the best action instead of a random action\n",
    "#     epsilon = 0.9 # 90% time select the best action\n",
    "\n",
    "#     # discount factor for future state's q_values\n",
    "#     discount_factor = 0.9\n",
    "\n",
    "#     learning_rate = 0.9\n",
    "\n",
    "#     for episode in range(5):\n",
    "# #         time_steps = 0\n",
    "#         print(\"Episode:\",end=\" \")\n",
    "#         print(episode)\n",
    "#         while not is_terminal_state(agent.xcor(),agent.ycor()):\n",
    "\n",
    "#             # Select the action to take from the current state\n",
    "#             next_action_to_take = choose_next_action(agent.xcor(),agent.ycor(),epsilon)\n",
    "\n",
    "#             print(\"Next Action:\", end=\" \")\n",
    "#             print(next_action_to_take)\n",
    "\n",
    "#             # save the current state x and y coordinates\n",
    "#             curr_state_x, curr_state_y = agent.xcor(), agent.ycor()\n",
    "#             print(\"Agent's current location:\", end=\" \")\n",
    "#             print(curr_state_x)\n",
    "#             print(curr_state_y)\n",
    "#             # get the next state x and y coordinates\n",
    "#             # the next state is the state where the agent will go after taking the action indicated by 'next_action_to_take'\n",
    "#             next_state_x, next_state_y = get_next_state_location(curr_state_x,curr_state_y,next_action_to_take)\n",
    "            \n",
    "# #             if next_state_y >= 600:\n",
    "# #                 next_state_y = 599\n",
    "# #             if next_state_x >= 800:\n",
    "# #                 next_state_x = 799\n",
    "            \n",
    "#             # Get the reward to go to that next state\n",
    "#             reward = rewards[next_state_x,next_state_y]\n",
    "\n",
    "#             # Now calculate the current q-value of the current state for that 'next_action_to_take' from the current state\n",
    "#             current_q_value = q_values[curr_state_x,curr_state_y,next_action_to_take]\n",
    "\n",
    "#             # Now in the next state, there are 5 actions. So there are 5 q-values in the next state and we know those q-values.\n",
    "#             # We can not say now which action will be choosen from that next state in future, but based on the current q-values\n",
    "#             # of the next state, we can guess what can be the maximum q-value that we can achieve in that next state in future \n",
    "#             # time step. Let's get that maximum q-value in the next state.\n",
    "\n",
    "#             max_achievable_q_values_in_next_state = np.max(q_values[next_state_x,next_state_y])\n",
    "\n",
    "#             # We do not want to get the full effect of that knowledge to our current state. Let's minimize the effect.\n",
    "\n",
    "#             max_achievable_q_values_in_next_state = discount_factor * max_achievable_q_values_in_next_state\n",
    "\n",
    "#             # Now, let us calculate the overall profit the agent can get by going to that next state\n",
    "\n",
    "#             profit = reward + max_achievable_q_values_in_next_state\n",
    "\n",
    "#             # Let us calculate how the profit is compared to the profit in our curent state\n",
    "#             temporal_difference = profit - current_q_value\n",
    "\n",
    "#             # Let us update the current state profit or in other words, q-value. We do not want to be greedy.  \n",
    "#             # So, we will not learn 100%.\n",
    "#             new_q_value_of_the_current_state_for_the_next_action = current_q_value + (learning_rate * temporal_difference)\n",
    "#             q_values[curr_state_x,curr_state_y,next_action_to_take] = new_q_value_of_the_current_state_for_the_next_action\n",
    "\n",
    "#             # Update agents location now\n",
    "#             agent.setx(next_state_x)\n",
    "#             agent.sety(next_state_y)\n",
    "\n",
    "#         agent.setx(start_x)\n",
    "#         agent.sety(start_y)\n",
    "        \n",
    "#     agent.setx(400)\n",
    "#     agent.sety(150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
