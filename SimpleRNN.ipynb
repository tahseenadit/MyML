{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-9KOXW4TQLn",
        "outputId": "ec834201-6072-4669-9371-385fae2a292a"
      },
      "source": [
        "import statistics\r\n",
        "\r\n",
        "inputs = [1,2,3,4]\r\n",
        "\r\n",
        "initial_prediction = 0 # Guessed\r\n",
        "initial_weight = 0.5\r\n",
        "initial_bias = 0.1\r\n",
        "initial_loss = inputs[0] - initial_prediction\r\n",
        "\r\n",
        "w = initial_weight\r\n",
        "b = initial_bias\r\n",
        "output = initial_prediction\r\n",
        "losses = []\r\n",
        "losses.append(initial_loss)\r\n",
        "\r\n",
        "dw = 0.001\r\n",
        "avg_loss = 10000\r\n",
        "\r\n",
        "def hidden_node_function(input,prediction,weight,bias):\r\n",
        "  output_from_hidden_layer_neuron = weight*prediction + weight*input + bias\r\n",
        "  return output_from_hidden_layer_neuron\r\n",
        "\r\n",
        "def activation_function(output_from_hidden_layer_neuron):\r\n",
        "  output_from_hidden_layer_neuron_activation_function = output_from_hidden_layer_neuron\r\n",
        "  return output_from_hidden_layer_neuron_activation_function\r\n",
        "\r\n",
        "epochs = 0\r\n",
        "\r\n",
        "\r\n",
        "for i in range(len(inputs)-1):\r\n",
        "    x = inputs[i]\r\n",
        "    output_from_hidden_layer_neuron = hidden_node_function(x,output,w,b)\r\n",
        "    output_from_hidden_layer_neuron_activation_function = activation_function(output_from_hidden_layer_neuron)\r\n",
        "    output = output_from_hidden_layer_neuron_activation_function\r\n",
        "    loss = inputs[i+1] - output\r\n",
        "    losses.append(loss)\r\n",
        "new_avg_loss = statistics.mean(losses)\r\n",
        "w = w + dw \r\n",
        "avg_loss = new_avg_loss\r\n",
        "epochs = epochs + 1\r\n",
        "print(\"Epoch : \", epochs ,\", Avg loss : \", avg_loss)\r\n",
        "\r\n",
        "\r\n",
        "while avg_loss > 0.1:\r\n",
        "\r\n",
        "  for i in range(len(inputs)-1):\r\n",
        "    x = inputs[i]\r\n",
        "    output_from_hidden_layer_neuron = hidden_node_function(x,output,w,b)\r\n",
        "    output_from_hidden_layer_neuron_activation_function = activation_function(output_from_hidden_layer_neuron)\r\n",
        "    output = output_from_hidden_layer_neuron_activation_function\r\n",
        "    loss = inputs[i+1] - output\r\n",
        "    losses.append(loss)\r\n",
        "  new_avg_loss = statistics.mean(losses)\r\n",
        "\r\n",
        "  if new_avg_loss > 0.1:\r\n",
        "    if new_avg_loss < avg_loss:\r\n",
        "      w = w + dw\r\n",
        "    else:\r\n",
        "      w = w - dw\r\n",
        "\r\n",
        "  avg_loss = new_avg_loss\r\n",
        "  epochs = epochs + 1\r\n",
        "  print(\"Epoch : \", epochs ,\", Avg loss : \",avg_loss)\r\n",
        "\r\n",
        "### Prediction\r\n",
        "\r\n",
        "x = inputs[-1]\r\n",
        "output_from_hidden_layer_neuron = hidden_node_function(x,output,w,b)\r\n",
        "output_from_hidden_layer_neuron_activation_function = activation_function(output_from_hidden_layer_neuron)\r\n",
        "output = output_from_hidden_layer_neuron_activation_function\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Original output: \", 5)\r\n",
        "print(\"Predicted output: \", output)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  1 , Avg loss :  1.4249999999999998\n",
            "Epoch :  2 , Avg loss :  1.1958735209571427\n",
            "Epoch :  3 , Avg loss :  1.0765912468187357\n",
            "Epoch :  4 , Avg loss :  1.0079759063210478\n",
            "Epoch :  5 , Avg loss :  0.9632477260274447\n",
            "Epoch :  6 , Avg loss :  0.9312705437130628\n",
            "Epoch :  7 , Avg loss :  0.9068436232903434\n",
            "Epoch :  8 , Avg loss :  0.887247002788827\n",
            "Epoch :  9 , Avg loss :  0.8709246168797543\n",
            "Epoch :  10 , Avg loss :  0.8569225205849094\n",
            "Epoch :  11 , Avg loss :  0.8446234120824141\n",
            "Epoch :  12 , Avg loss :  0.8336101751609534\n",
            "Epoch :  13 , Avg loss :  0.8235908118400225\n",
            "Epoch :  14 , Avg loss :  0.8143547966390166\n",
            "Epoch :  15 , Avg loss :  0.80574650937637\n",
            "Epoch :  16 , Avg loss :  0.797648427318672\n",
            "Epoch :  17 , Avg loss :  0.7899701354273461\n",
            "Epoch :  18 , Avg loss :  0.7826409332503944\n",
            "Epoch :  19 , Avg loss :  0.7756047362260101\n",
            "Epoch :  20 , Avg loss :  0.7688164815185955\n",
            "Epoch :  21 , Avg loss :  0.7622395447124877\n",
            "Epoch :  22 , Avg loss :  0.7558438505273826\n",
            "Epoch :  23 , Avg loss :  0.7496044693489379\n",
            "Epoch :  24 , Avg loss :  0.7435005598194929\n",
            "Epoch :  25 , Avg loss :  0.7375145618670211\n",
            "Epoch :  26 , Avg loss :  0.7316315736001174\n",
            "Epoch :  27 , Avg loss :  0.7258388649813671\n",
            "Epoch :  28 , Avg loss :  0.7201254944867819\n",
            "Epoch :  29 , Avg loss :  0.7144820041750725\n",
            "Epoch :  30 , Avg loss :  0.7089001750721721\n",
            "Epoch :  31 , Avg loss :  0.7033728293963171\n",
            "Epoch :  32 , Avg loss :  0.6978936694829383\n",
            "Epoch :  33 , Avg loss :  0.6924571457023938\n",
            "Epoch :  34 , Avg loss :  0.6870583474593717\n",
            "Epoch :  35 , Avg loss :  0.6816929127011732\n",
            "Epoch :  36 , Avg loss :  0.6763569523689353\n",
            "Epoch :  37 , Avg loss :  0.6710469869899842\n",
            "Epoch :  38 , Avg loss :  0.6657598931942347\n",
            "Epoch :  39 , Avg loss :  0.6604928583884857\n",
            "Epoch :  40 , Avg loss :  0.6552433421727691\n",
            "Epoch :  41 , Avg loss :  0.6500090433569461\n",
            "Epoch :  42 , Avg loss :  0.6447878716515166\n",
            "Epoch :  43 , Avg loss :  0.6395779232775718\n",
            "Epoch :  44 , Avg loss :  0.6343774598770662\n",
            "Epoch :  45 , Avg loss :  0.6291848902137981\n",
            "Epoch :  46 , Avg loss :  0.6239987542434691\n",
            "Epoch :  47 , Avg loss :  0.6188177092024613\n",
            "Epoch :  48 , Avg loss :  0.6136405174229578\n",
            "Epoch :  49 , Avg loss :  0.6084660356294447\n",
            "Epoch :  50 , Avg loss :  0.6032932055105685\n",
            "Epoch :  51 , Avg loss :  0.5981210453924286\n",
            "Epoch :  52 , Avg loss :  0.5929486428659723\n",
            "Epoch :  53 , Avg loss :  0.587775148243259\n",
            "Epoch :  54 , Avg loss :  0.5825997687358007\n",
            "Epoch :  55 , Avg loss :  0.5774217632636225\n",
            "Epoch :  56 , Avg loss :  0.5722404378166647\n",
            "Epoch :  57 , Avg loss :  0.5670551413010811\n",
            "Epoch :  58 , Avg loss :  0.5618652618122394\n",
            "Epoch :  59 , Avg loss :  0.5566702232840748\n",
            "Epoch :  60 , Avg loss :  0.5514694824711247\n",
            "Epoch :  61 , Avg loss :  0.5462625262252695\n",
            "Epoch :  62 , Avg loss :  0.5410488690340745\n",
            "Epoch :  63 , Avg loss :  0.5358280507918152\n",
            "Epoch :  64 , Avg loss :  0.5305996347778585\n",
            "Epoch :  65 , Avg loss :  0.5253632058201776\n",
            "Epoch :  66 , Avg loss :  0.5201183686244556\n",
            "Epoch :  67 , Avg loss :  0.5148647462515574\n",
            "Epoch :  68 , Avg loss :  0.5096019787281634\n",
            "Epoch :  69 , Avg loss :  0.504329721777113\n",
            "Epoch :  70 , Avg loss :  0.49904764565553833\n",
            "Epoch :  71 , Avg loss :  0.4937554340902017\n",
            "Epoch :  72 , Avg loss :  0.4884527833006239\n",
            "Epoch :  73 , Avg loss :  0.48313940110161646\n",
            "Epoch :  74 , Avg loss :  0.4778150060777316\n",
            "Epoch :  75 , Avg loss :  0.472479326822942\n",
            "Epoch :  76 , Avg loss :  0.4671321012395608\n",
            "Epoch :  77 , Avg loss :  0.46177307589103067\n",
            "Epoch :  78 , Avg loss :  0.45640200540376435\n",
            "Epoch :  79 , Avg loss :  0.45101865191369755\n",
            "Epoch :  80 , Avg loss :  0.4456227845536535\n",
            "Epoch :  81 , Avg loss :  0.44021417897799825\n",
            "Epoch :  82 , Avg loss :  0.4347926169214087\n",
            "Epoch :  83 , Avg loss :  0.4293578857888813\n",
            "Epoch :  84 , Avg loss :  0.4239097782743801\n",
            "Epoch :  85 , Avg loss :  0.41844809200576794\n",
            "Epoch :  86 , Avg loss :  0.41297262921388184\n",
            "Epoch :  87 , Avg loss :  0.40748319642381026\n",
            "Epoch :  88 , Avg loss :  0.4019796041666044\n",
            "Epoch :  89 , Avg loss :  0.3964616667098161\n",
            "Epoch :  90 , Avg loss :  0.3909292018053947\n",
            "Epoch :  91 , Avg loss :  0.3853820304536049\n",
            "Epoch :  92 , Avg loss :  0.3798199766817439\n",
            "Epoch :  93 , Avg loss :  0.37424286733653916\n",
            "Epoch :  94 , Avg loss :  0.36865053188920494\n",
            "Epoch :  95 , Avg loss :  0.36304280225221897\n",
            "Epoch :  96 , Avg loss :  0.35741951260696053\n",
            "Epoch :  97 , Avg loss :  0.3517804992414206\n",
            "Epoch :  98 , Avg loss :  0.3461256003972591\n",
            "Epoch :  99 , Avg loss :  0.3404546561255422\n",
            "Epoch :  100 , Avg loss :  0.33476750815054623\n",
            "Epoch :  101 , Avg loss :  0.3290639997410624\n",
            "Epoch :  102 , Avg loss :  0.3233439755886817\n",
            "Epoch :  103 , Avg loss :  0.3176072816925779\n",
            "Epoch :  104 , Avg loss :  0.3118537652503445\n",
            "Epoch :  105 , Avg loss :  0.30608327455447554\n",
            "Epoch :  106 , Avg loss :  0.3002956588941096\n",
            "Epoch :  107 , Avg loss :  0.29449076846168437\n",
            "Epoch :  108 , Avg loss :  0.28866845426417814\n",
            "Epoch :  109 , Avg loss :  0.2828285680386328\n",
            "Epoch :  110 , Avg loss :  0.27697096217168077\n",
            "Epoch :  111 , Avg loss :  0.27109548962281255\n",
            "Epoch :  112 , Avg loss :  0.26520200385114545\n",
            "Epoch :  113 , Avg loss :  0.259290358745466\n",
            "Epoch :  114 , Avg loss :  0.2533604085573377\n",
            "Epoch :  115 , Avg loss :  0.24741200783707865\n",
            "Epoch :  116 , Avg loss :  0.24144501137242666\n",
            "Epoch :  117 , Avg loss :  0.23545927412972295\n",
            "Epoch :  118 , Avg loss :  0.2294546511974551\n",
            "Epoch :  119 , Avg loss :  0.22343099773201158\n",
            "Epoch :  120 , Avg loss :  0.21738816890551024\n",
            "Epoch :  121 , Avg loss :  0.2113260198555702\n",
            "Epoch :  122 , Avg loss :  0.205244405636907\n",
            "Epoch :  123 , Avg loss :  0.1991431811746372\n",
            "Epoch :  124 , Avg loss :  0.19302220121918637\n",
            "Epoch :  125 , Avg loss :  0.18688132030270052\n",
            "Epoch :  126 , Avg loss :  0.1807203926968675\n",
            "Epoch :  127 , Avg loss :  0.1745392723720608\n",
            "Epoch :  128 , Avg loss :  0.1683378129577228\n",
            "Epoch :  129 , Avg loss :  0.16211586770390987\n",
            "Epoch :  130 , Avg loss :  0.15587328944392645\n",
            "Epoch :  131 , Avg loss :  0.14960993055797916\n",
            "Epoch :  132 , Avg loss :  0.14332564293778594\n",
            "Epoch :  133 , Avg loss :  0.13702027795207916\n",
            "Epoch :  134 , Avg loss :  0.13069368641294515\n",
            "Epoch :  135 , Avg loss :  0.12434571854294516\n",
            "Epoch :  136 , Avg loss :  0.11797622394296657\n",
            "Epoch :  137 , Avg loss :  0.1115850515607556\n",
            "Epoch :  138 , Avg loss :  0.105172049660085\n",
            "Epoch :  139 , Avg loss :  0.09873706579051347\n",
            "\n",
            "Original output:  5\n",
            "Predicted output:  5.399447629090172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr3h_yrflxMQ"
      },
      "source": [
        "But in the above code, we are using a single cell. We are feeding the output from that single cell to the next timestep along with the next input. Let us now desing the model for multiple cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWuYasnoThjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44cf2f5e-982d-4169-c83d-791b4733b0d5"
      },
      "source": [
        "import statistics\r\n",
        "\r\n",
        "inputs = [1,2,3,4]\r\n",
        "\r\n",
        "initial_prediction = 0 # Guessed\r\n",
        "initial_weight = 0.5\r\n",
        "initial_bias = 0.1\r\n",
        "initial_loss = inputs[0] - initial_prediction\r\n",
        "\r\n",
        "w = initial_weight\r\n",
        "b = initial_bias\r\n",
        "output = initial_prediction\r\n",
        "losses = []\r\n",
        "losses.append(initial_loss)\r\n",
        "\r\n",
        "dw = 0.001\r\n",
        "avg_loss = 10000\r\n",
        "\r\n",
        "output_from_hidden_layer_neuron = [0,0,0,0,0]\r\n",
        "\r\n",
        "def hidden_node_function(input,prediction,weight,bias):\r\n",
        "  output_from_hidden_layer_neuron = weight*prediction + weight*input + bias\r\n",
        "  return output_from_hidden_layer_neuron\r\n",
        "\r\n",
        "def activation_function(output_from_hidden_layer_neuron):\r\n",
        "  output_from_hidden_layer_neuron_activation_function = output_from_hidden_layer_neuron\r\n",
        "  return output_from_hidden_layer_neuron_activation_function\r\n",
        "\r\n",
        "epochs = 0\r\n",
        "\r\n",
        "\r\n",
        "for i in range(len(inputs)-1):\r\n",
        "    x = inputs[i]\r\n",
        "    output_from_hidden_layer_neuron[i+1] = hidden_node_function(x,output_from_hidden_layer_neuron[i],w,b)\r\n",
        "    output_from_hidden_layer_neuron_activation_function = activation_function(output_from_hidden_layer_neuron[i+1])\r\n",
        "    output = output_from_hidden_layer_neuron_activation_function\r\n",
        "    loss = inputs[i+1] - output\r\n",
        "    # losses.append(loss)\r\n",
        "new_avg_loss = loss\r\n",
        "w = w + dw \r\n",
        "avg_loss = new_avg_loss\r\n",
        "epochs = epochs + 1\r\n",
        "print(\"Epoch : \", epochs ,\", Avg loss : \", avg_loss)\r\n",
        "\r\n",
        "\r\n",
        "while avg_loss > 0.1:\r\n",
        "\r\n",
        "  for i in range(len(inputs)-1):\r\n",
        "    x = inputs[i]\r\n",
        "    output_from_hidden_layer_neuron[i+1] = hidden_node_function(x,output_from_hidden_layer_neuron[i],w,b)\r\n",
        "    output_from_hidden_layer_neuron_activation_function = activation_function(output_from_hidden_layer_neuron[i+1])\r\n",
        "    output = output_from_hidden_layer_neuron_activation_function\r\n",
        "    loss = inputs[i+1] - output\r\n",
        "    # losses.append(loss)\r\n",
        "  new_avg_loss = loss\r\n",
        "\r\n",
        "  if new_avg_loss > 0.1:\r\n",
        "    if new_avg_loss < avg_loss:\r\n",
        "      w = w + dw\r\n",
        "    else:\r\n",
        "      w = w - dw\r\n",
        "\r\n",
        "  avg_loss = new_avg_loss\r\n",
        "  epochs = epochs + 1\r\n",
        "  print(\"Epoch : \", epochs ,\", Avg loss : \",avg_loss)\r\n",
        "\r\n",
        "### Prediction\r\n",
        "\r\n",
        "x = inputs[-1]\r\n",
        "output_from_hidden_layer_neuron[-1] = hidden_node_function(x,output_from_hidden_layer_neuron[-2],w,b)\r\n",
        "output_from_hidden_layer_neuron_activation_function = activation_function(output_from_hidden_layer_neuron[-1])\r\n",
        "output = output_from_hidden_layer_neuron_activation_function\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Original output: \", 5)\r\n",
        "print(\"Predicted output: \", output)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  1 , Avg loss :  1.6999999999999997\n",
            "Epoch :  2 , Avg loss :  1.6940463989999999\n",
            "Epoch :  3 , Avg loss :  1.6880855919999997\n",
            "Epoch :  4 , Avg loss :  1.6821175729999998\n",
            "Epoch :  5 , Avg loss :  1.6761423359999998\n",
            "Epoch :  6 , Avg loss :  1.670159875\n",
            "Epoch :  7 , Avg loss :  1.6641701839999996\n",
            "Epoch :  8 , Avg loss :  1.658173257\n",
            "Epoch :  9 , Avg loss :  1.652169088\n",
            "Epoch :  10 , Avg loss :  1.6461576709999997\n",
            "Epoch :  11 , Avg loss :  1.640139\n",
            "Epoch :  12 , Avg loss :  1.634113069\n",
            "Epoch :  13 , Avg loss :  1.6280798719999998\n",
            "Epoch :  14 , Avg loss :  1.6220394029999996\n",
            "Epoch :  15 , Avg loss :  1.6159916559999998\n",
            "Epoch :  16 , Avg loss :  1.609936625\n",
            "Epoch :  17 , Avg loss :  1.603874304\n",
            "Epoch :  18 , Avg loss :  1.5978046869999996\n",
            "Epoch :  19 , Avg loss :  1.5917277679999997\n",
            "Epoch :  20 , Avg loss :  1.585643541\n",
            "Epoch :  21 , Avg loss :  1.5795519999999996\n",
            "Epoch :  22 , Avg loss :  1.5734531389999997\n",
            "Epoch :  23 , Avg loss :  1.5673469519999998\n",
            "Epoch :  24 , Avg loss :  1.561233433\n",
            "Epoch :  25 , Avg loss :  1.5551125759999995\n",
            "Epoch :  26 , Avg loss :  1.5489843749999994\n",
            "Epoch :  27 , Avg loss :  1.5428488239999996\n",
            "Epoch :  28 , Avg loss :  1.536705917\n",
            "Epoch :  29 , Avg loss :  1.5305556479999995\n",
            "Epoch :  30 , Avg loss :  1.5243980109999997\n",
            "Epoch :  31 , Avg loss :  1.518233\n",
            "Epoch :  32 , Avg loss :  1.5120606089999997\n",
            "Epoch :  33 , Avg loss :  1.5058808319999994\n",
            "Epoch :  34 , Avg loss :  1.4996936629999995\n",
            "Epoch :  35 , Avg loss :  1.4934990959999994\n",
            "Epoch :  36 , Avg loss :  1.487297125\n",
            "Epoch :  37 , Avg loss :  1.4810877439999994\n",
            "Epoch :  38 , Avg loss :  1.4748709469999994\n",
            "Epoch :  39 , Avg loss :  1.4686467279999995\n",
            "Epoch :  40 , Avg loss :  1.4624150809999996\n",
            "Epoch :  41 , Avg loss :  1.4561759999999997\n",
            "Epoch :  42 , Avg loss :  1.4499294789999997\n",
            "Epoch :  43 , Avg loss :  1.4436755119999996\n",
            "Epoch :  44 , Avg loss :  1.4374140929999997\n",
            "Epoch :  45 , Avg loss :  1.4311452159999996\n",
            "Epoch :  46 , Avg loss :  1.4248688749999996\n",
            "Epoch :  47 , Avg loss :  1.4185850639999997\n",
            "Epoch :  48 , Avg loss :  1.412293777\n",
            "Epoch :  49 , Avg loss :  1.4059950079999997\n",
            "Epoch :  50 , Avg loss :  1.3996887509999998\n",
            "Epoch :  51 , Avg loss :  1.3933749999999998\n",
            "Epoch :  52 , Avg loss :  1.3870537489999997\n",
            "Epoch :  53 , Avg loss :  1.3807249919999998\n",
            "Epoch :  54 , Avg loss :  1.3743887229999996\n",
            "Epoch :  55 , Avg loss :  1.3680449359999995\n",
            "Epoch :  56 , Avg loss :  1.3616936249999996\n",
            "Epoch :  57 , Avg loss :  1.3553347839999996\n",
            "Epoch :  58 , Avg loss :  1.3489684069999996\n",
            "Epoch :  59 , Avg loss :  1.3425944879999996\n",
            "Epoch :  60 , Avg loss :  1.3362130209999994\n",
            "Epoch :  61 , Avg loss :  1.3298239999999995\n",
            "Epoch :  62 , Avg loss :  1.3234274189999993\n",
            "Epoch :  63 , Avg loss :  1.3170232719999997\n",
            "Epoch :  64 , Avg loss :  1.3106115529999998\n",
            "Epoch :  65 , Avg loss :  1.3041922559999994\n",
            "Epoch :  66 , Avg loss :  1.2977653749999996\n",
            "Epoch :  67 , Avg loss :  1.2913309039999996\n",
            "Epoch :  68 , Avg loss :  1.2848888369999996\n",
            "Epoch :  69 , Avg loss :  1.2784391679999998\n",
            "Epoch :  70 , Avg loss :  1.2719818909999994\n",
            "Epoch :  71 , Avg loss :  1.2655169999999996\n",
            "Epoch :  72 , Avg loss :  1.2590444889999994\n",
            "Epoch :  73 , Avg loss :  1.2525643519999994\n",
            "Epoch :  74 , Avg loss :  1.2460765829999993\n",
            "Epoch :  75 , Avg loss :  1.2395811759999993\n",
            "Epoch :  76 , Avg loss :  1.2330781249999996\n",
            "Epoch :  77 , Avg loss :  1.2265674239999993\n",
            "Epoch :  78 , Avg loss :  1.2200490669999993\n",
            "Epoch :  79 , Avg loss :  1.2135230479999994\n",
            "Epoch :  80 , Avg loss :  1.2069893609999993\n",
            "Epoch :  81 , Avg loss :  1.2004479999999993\n",
            "Epoch :  82 , Avg loss :  1.1938989589999993\n",
            "Epoch :  83 , Avg loss :  1.1873422319999993\n",
            "Epoch :  84 , Avg loss :  1.1807778129999993\n",
            "Epoch :  85 , Avg loss :  1.1742056959999991\n",
            "Epoch :  86 , Avg loss :  1.1676258749999993\n",
            "Epoch :  87 , Avg loss :  1.1610383439999992\n",
            "Epoch :  88 , Avg loss :  1.1544430969999993\n",
            "Epoch :  89 , Avg loss :  1.1478401279999995\n",
            "Epoch :  90 , Avg loss :  1.1412294309999993\n",
            "Epoch :  91 , Avg loss :  1.1346109999999991\n",
            "Epoch :  92 , Avg loss :  1.1279848289999994\n",
            "Epoch :  93 , Avg loss :  1.1213509119999991\n",
            "Epoch :  94 , Avg loss :  1.1147092429999992\n",
            "Epoch :  95 , Avg loss :  1.1080598159999995\n",
            "Epoch :  96 , Avg loss :  1.101402624999999\n",
            "Epoch :  97 , Avg loss :  1.0947376639999993\n",
            "Epoch :  98 , Avg loss :  1.0880649269999991\n",
            "Epoch :  99 , Avg loss :  1.0813844079999995\n",
            "Epoch :  100 , Avg loss :  1.0746961009999993\n",
            "Epoch :  101 , Avg loss :  1.0679999999999992\n",
            "Epoch :  102 , Avg loss :  1.0612960989999993\n",
            "Epoch :  103 , Avg loss :  1.0545843919999993\n",
            "Epoch :  104 , Avg loss :  1.0478648729999995\n",
            "Epoch :  105 , Avg loss :  1.0411375359999995\n",
            "Epoch :  106 , Avg loss :  1.034402374999999\n",
            "Epoch :  107 , Avg loss :  1.0276593839999992\n",
            "Epoch :  108 , Avg loss :  1.0209085569999994\n",
            "Epoch :  109 , Avg loss :  1.014149887999999\n",
            "Epoch :  110 , Avg loss :  1.007383370999999\n",
            "Epoch :  111 , Avg loss :  1.0006089999999994\n",
            "Epoch :  112 , Avg loss :  0.9938267689999996\n",
            "Epoch :  113 , Avg loss :  0.987036671999999\n",
            "Epoch :  114 , Avg loss :  0.980238702999999\n",
            "Epoch :  115 , Avg loss :  0.9734328559999992\n",
            "Epoch :  116 , Avg loss :  0.9666191249999994\n",
            "Epoch :  117 , Avg loss :  0.9597975039999991\n",
            "Epoch :  118 , Avg loss :  0.9529679869999987\n",
            "Epoch :  119 , Avg loss :  0.9461305679999992\n",
            "Epoch :  120 , Avg loss :  0.939285240999999\n",
            "Epoch :  121 , Avg loss :  0.932431999999999\n",
            "Epoch :  122 , Avg loss :  0.9255708389999993\n",
            "Epoch :  123 , Avg loss :  0.9187017519999991\n",
            "Epoch :  124 , Avg loss :  0.9118247329999991\n",
            "Epoch :  125 , Avg loss :  0.9049397759999991\n",
            "Epoch :  126 , Avg loss :  0.898046874999999\n",
            "Epoch :  127 , Avg loss :  0.8911460239999989\n",
            "Epoch :  128 , Avg loss :  0.884237216999999\n",
            "Epoch :  129 , Avg loss :  0.877320447999999\n",
            "Epoch :  130 , Avg loss :  0.8703957109999991\n",
            "Epoch :  131 , Avg loss :  0.863462999999999\n",
            "Epoch :  132 , Avg loss :  0.8565223089999994\n",
            "Epoch :  133 , Avg loss :  0.8495736319999989\n",
            "Epoch :  134 , Avg loss :  0.8426169629999989\n",
            "Epoch :  135 , Avg loss :  0.8356522959999988\n",
            "Epoch :  136 , Avg loss :  0.828679624999999\n",
            "Epoch :  137 , Avg loss :  0.8216989439999991\n",
            "Epoch :  138 , Avg loss :  0.8147102469999989\n",
            "Epoch :  139 , Avg loss :  0.8077135279999994\n",
            "Epoch :  140 , Avg loss :  0.8007087809999991\n",
            "Epoch :  141 , Avg loss :  0.7936959999999988\n",
            "Epoch :  142 , Avg loss :  0.7866751789999991\n",
            "Epoch :  143 , Avg loss :  0.7796463119999992\n",
            "Epoch :  144 , Avg loss :  0.7726093929999993\n",
            "Epoch :  145 , Avg loss :  0.7655644159999988\n",
            "Epoch :  146 , Avg loss :  0.758511374999999\n",
            "Epoch :  147 , Avg loss :  0.7514502639999989\n",
            "Epoch :  148 , Avg loss :  0.744381076999999\n",
            "Epoch :  149 , Avg loss :  0.7373038079999987\n",
            "Epoch :  150 , Avg loss :  0.7302184509999989\n",
            "Epoch :  151 , Avg loss :  0.7231249999999991\n",
            "Epoch :  152 , Avg loss :  0.7160234489999993\n",
            "Epoch :  153 , Avg loss :  0.7089137919999993\n",
            "Epoch :  154 , Avg loss :  0.7017960229999987\n",
            "Epoch :  155 , Avg loss :  0.6946701359999987\n",
            "Epoch :  156 , Avg loss :  0.6875361249999989\n",
            "Epoch :  157 , Avg loss :  0.6803939839999988\n",
            "Epoch :  158 , Avg loss :  0.6732437069999988\n",
            "Epoch :  159 , Avg loss :  0.6660852879999988\n",
            "Epoch :  160 , Avg loss :  0.6589187209999987\n",
            "Epoch :  161 , Avg loss :  0.6517439999999985\n",
            "Epoch :  162 , Avg loss :  0.6445611189999987\n",
            "Epoch :  163 , Avg loss :  0.6373700719999986\n",
            "Epoch :  164 , Avg loss :  0.6301708529999988\n",
            "Epoch :  165 , Avg loss :  0.6229634559999986\n",
            "Epoch :  166 , Avg loss :  0.615747874999999\n",
            "Epoch :  167 , Avg loss :  0.6085241039999985\n",
            "Epoch :  168 , Avg loss :  0.6012921369999993\n",
            "Epoch :  169 , Avg loss :  0.5940519679999987\n",
            "Epoch :  170 , Avg loss :  0.5868035909999985\n",
            "Epoch :  171 , Avg loss :  0.5795469999999985\n",
            "Epoch :  172 , Avg loss :  0.5722821889999987\n",
            "Epoch :  173 , Avg loss :  0.5650091519999987\n",
            "Epoch :  174 , Avg loss :  0.5577278829999988\n",
            "Epoch :  175 , Avg loss :  0.5504383759999989\n",
            "Epoch :  176 , Avg loss :  0.543140624999999\n",
            "Epoch :  177 , Avg loss :  0.5358346239999987\n",
            "Epoch :  178 , Avg loss :  0.5285203669999987\n",
            "Epoch :  179 , Avg loss :  0.5211978479999986\n",
            "Epoch :  180 , Avg loss :  0.5138670609999987\n",
            "Epoch :  181 , Avg loss :  0.506527999999999\n",
            "Epoch :  182 , Avg loss :  0.4991806589999985\n",
            "Epoch :  183 , Avg loss :  0.49182503199999905\n",
            "Epoch :  184 , Avg loss :  0.4844611129999987\n",
            "Epoch :  185 , Avg loss :  0.4770888959999984\n",
            "Epoch :  186 , Avg loss :  0.4697083749999984\n",
            "Epoch :  187 , Avg loss :  0.4623195439999983\n",
            "Epoch :  188 , Avg loss :  0.4549223969999989\n",
            "Epoch :  189 , Avg loss :  0.44751692799999887\n",
            "Epoch :  190 , Avg loss :  0.44010313099999854\n",
            "Epoch :  191 , Avg loss :  0.4326809999999992\n",
            "Epoch :  192 , Avg loss :  0.42525052899999904\n",
            "Epoch :  193 , Avg loss :  0.41781171199999845\n",
            "Epoch :  194 , Avg loss :  0.4103645429999987\n",
            "Epoch :  195 , Avg loss :  0.4029090159999984\n",
            "Epoch :  196 , Avg loss :  0.39544512499999884\n",
            "Epoch :  197 , Avg loss :  0.38797286399999864\n",
            "Epoch :  198 , Avg loss :  0.38049222699999863\n",
            "Epoch :  199 , Avg loss :  0.37300320799999875\n",
            "Epoch :  200 , Avg loss :  0.3655058009999985\n",
            "Epoch :  201 , Avg loss :  0.35799999999999876\n",
            "Epoch :  202 , Avg loss :  0.35048579899999854\n",
            "Epoch :  203 , Avg loss :  0.34296319199999825\n",
            "Epoch :  204 , Avg loss :  0.3354321729999987\n",
            "Epoch :  205 , Avg loss :  0.3278927359999986\n",
            "Epoch :  206 , Avg loss :  0.32034487499999864\n",
            "Epoch :  207 , Avg loss :  0.31278858399999887\n",
            "Epoch :  208 , Avg loss :  0.3052238569999983\n",
            "Epoch :  209 , Avg loss :  0.2976506879999987\n",
            "Epoch :  210 , Avg loss :  0.29006907099999824\n",
            "Epoch :  211 , Avg loss :  0.28247899999999815\n",
            "Epoch :  212 , Avg loss :  0.27488046899999885\n",
            "Epoch :  213 , Avg loss :  0.26727347199999807\n",
            "Epoch :  214 , Avg loss :  0.2596580029999984\n",
            "Epoch :  215 , Avg loss :  0.25203405599999895\n",
            "Epoch :  216 , Avg loss :  0.24440162499999873\n",
            "Epoch :  217 , Avg loss :  0.2367607039999986\n",
            "Epoch :  218 , Avg loss :  0.22911128699999805\n",
            "Epoch :  219 , Avg loss :  0.22145336799999837\n",
            "Epoch :  220 , Avg loss :  0.21378694099999818\n",
            "Epoch :  221 , Avg loss :  0.2061119999999983\n",
            "Epoch :  222 , Avg loss :  0.19842853899999868\n",
            "Epoch :  223 , Avg loss :  0.1907365519999984\n",
            "Epoch :  224 , Avg loss :  0.18303603299999827\n",
            "Epoch :  225 , Avg loss :  0.1753269759999987\n",
            "Epoch :  226 , Avg loss :  0.16760937499999828\n",
            "Epoch :  227 , Avg loss :  0.15988322399999833\n",
            "Epoch :  228 , Avg loss :  0.15214851699999832\n",
            "Epoch :  229 , Avg loss :  0.14440524799999865\n",
            "Epoch :  230 , Avg loss :  0.13665341099999795\n",
            "Epoch :  231 , Avg loss :  0.12889299999999837\n",
            "Epoch :  232 , Avg loss :  0.12112400899999853\n",
            "Epoch :  233 , Avg loss :  0.11334643199999839\n",
            "Epoch :  234 , Avg loss :  0.10556026299999788\n",
            "Epoch :  235 , Avg loss :  0.09776549599999784\n",
            "\n",
            "Original output:  5\n",
            "Predicted output:  5.900240125936003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCQt5jcXtRxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}